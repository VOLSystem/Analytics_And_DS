---
title: "Austin Animal Shelter Outcomes"
author: "You"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,collapse=TRUE)
options(width=120)
library(regclass)
library(lubridate)
library(discretization)
library(regclass)
library(caret)
library(caretEnsemble)
library(nnet)
library(e1071)
library(glmnet)
library(gbm)
library(pROC)

####################################################################
####################################################################
###Combining Infrequent Levels
####################################################################
####################################################################

#Takes a categorical variable x and returns a list object containing a vector names
#of all levels that were combined ($combined component) and a vector of "new" levels ($values component)
#The threshold argument says "all levels that appear threshold times of fewer are combined"


# The `combine_infrequent_levels` function below (do not use the related `combine_rare_levels` in R, which has a bug that affects a very specific combination of levels) helps to combine levels that do not appear enough in the data to do reliable analytics.
# 
# * `x` - a factor / categorical variable where you want to combine levels
# 
# * `threshold` - any level appearing `threshold` or fewer times in the data will be combined.  Default is 20.  Don't be afraid to use 50 or even 100 for larger datasets.  With 100 instances of a level, you've narrowed down the probability of some trait of interest among that level to only plus or minus 10 percentage points!
#   
#   * `newname` - the name you want to call levels that have been combined together.  Default is "Combined"
# 
# Note 1:  when there is only ONE infrequent level, that level will be combined with the second most infrequent level and those two levels will be renamed "Combined".
# 
# Note 2:  this function returns a list object where the `$values` component gives the recoded values of `x`, and the `$combined` component gives which levels have been combined into "Combined"



combine_infrequent_levels <- function(x,threshold=20,newname="Combined") { 
  x <- factor(x)
  rare.levels <- names( which( sort( table(x) ) <= threshold ) )
  if(length(rare.levels)==0) { return(list(values=x,combined=NULL)) }
  
  levels(x)[ which(levels(x) %in% rare.levels) ] <- newname
  ST <- sort(table(x))
  if(ST[newname]<=threshold) {  #Combined will be the least frequent level
    levels.to.combine <- which( levels(x) %in% c(newname,names(ST)[2]))
    levels(x)[levels.to.combine] <- newname
    rare.levels <- c(rare.levels,names(ST)[2])}
  return(list(values=x,combined=rare.levels))
}


####################################################################
####################################################################
###Supervised Discretization using mdlp (minimum description length)
####################################################################
####################################################################


# The `discretize_x_for_categorical_y` function will propose an "intelligent" set of levels of `x` (which can be numerical 
#or categorical) based on how similar they behave in terms of a two-level categorical variable `y`. 
# 
# * `DATA` - a dataframe whose first column MUST contain the values of `x` (numerical or categorical) and whose second 
#            column MUST contain the values of `y` (must be a two-level categorical variable). 
# 
# * `threshold` - if `x` is a categorical variable and `combine_infrequent_levels` should be run simultaneously, 
#             change `threshold` to what you'd want it to be.  Leave it at 0 leaves everything unchanged.
# 
# * `train.rows` - if not `NA`, it should be a vector of row numbers that you want to use in the discretization.  
#                   If predictive analytics will eventually take place, it is a good idea to determine which rows will be going 
#                   into the training and holdout, do the discretization on all the data (but using only the information about 
#                   `y` in the rows that will be in the training set), then split into training/holdout.
# 
# * `equal` - controls the appearance of the mosaic plot illustrating the discretization.  If `TRUE` each level's segment 
#             will be equal width.  If `FALSE` each level's segment will reflect its underlying frequency in the data.
# 
# The output of the function is a list object.  The `$Details` component provides a translator between original values 
# and new levels.  The `$newlevels` gives a vector of each's values new representation in `DATA`.

discretize_x_for_categorical_y <- function(DATA,threshold=0,train.rows=NA,equal=FALSE) {
  require(discretization)
  require(regclass)
  if(class(DATA[,1]) %in% c("character","factor")) {
    if(threshold == 0 | is.na(threshold)) { } else { 
      DATA[,1] <- combine_infrequent_levels(DATA[,1],threshold)$values }
    if( length(train.rows)>1 ) { HALFDISC <- DATA[train.rows,] } else { HALFDISC <- DATA }
    A <- aggregate(HALFDISC[,2]~HALFDISC[,1],FUN=function(x)mean(x==levels(x)[1])) 
    A <- A[order(A[,2]),]
    SUB <- HALFDISC
    SUB[,1] <- match(HALFDISC[,1],A[,1]) 
    disc.scheme <- mdlp(SUB)
    cutoffs <- sort( unlist( disc.scheme$cutp ) )
    A$value <- 1:nrow(A)
    if(cutoffs[1] != "All") {
      A$code <- factor( rep(letters[1:(length(cutoffs)+1)],nrow(A)) )[1:nrow(A)] 
      
      for (i in 1:length(cutoffs)) {
        if(i==1) { A$code[1:floor(cutoffs[1])] <- letters[1] } else { 
          A$code[ which(A$value > cutoffs[i-1] & A$value <= cutoffs[i]) ] <- letters[i] }
      }
    A$code[ which(A$value>max(cutoffs)) ] <- letters[i+1]
    names(A) <- c( "OldValue","yAverage","Rank","NewValue")
    results <- list(Details=A,newlevels= factor( A$NewValue[ match(DATA[,1],A[,1]) ] ) )
    y <- DATA[,2]
    x <- results$newlevels
    TEMP <- data.frame(y,x)
    
    mosaic(y~x,data=TEMP,xlab="New Levels",ylab=names(DATA)[2],inside=TRUE,equal=equal) } else {
      names(A) <- c( "OldValue","yAverage","Rank")
      A$Rank <- order(A$yAverage)
      A$NewValue <- factor(rep("A",nrow(A)))
      results <- list(Details=A,newlevels= factor( rep("A",nrow(A)) ) )
    }
    
    return( results ) } else {
      
      
      
      if( length(train.rows)>1) { HALFDISC <- DATA[train.rows,] } else { HALFDISC <- DATA }
      SUB <- HALFDISC
      disc.scheme <- mdlp(SUB)
      cuts <- unlist( disc.scheme$cutp )
      A <- aggregate(disc.scheme$Disc.data[,2]~disc.scheme$Disc.data[,1],FUN=function(x)mean(x==levels(x)[1]))
      details <- data.frame(Lower=rep(0,nrow(A)),Upper=rep(0,nrow(A)),yAverage=rep(0,nrow(A)),NewValue=rep(0,nrow(A)))
      
      for (i in 1:nrow(details)) {
        details$Lower[i] <- min( SUB[ which(disc.scheme$Disc.data[,1]==i),1] )
        details$Upper[i] <- max( SUB[ which(disc.scheme$Disc.data[,1]==i),1] )
        details$NewValue[i] <- letters[i]
      }
      details$yAverage <- A[,2]
      newvalues <- rep(0,nrow(DATA)) 
      for (i in 1:nrow(DATA)) {
        temp <- which( DATA[i,1] >= details$Lower )
        if(length(temp)>0) { temp <- max( which( DATA[i,1] >= details$Lower ) ) } else { temp <- 1 }
        newvalues[i] <- details$NewValue[temp]
      }
      if(cuts[1]!="All") { 
      results <- list(Details=details,newlevels=factor(newvalues))
      y <- DATA[,2]
      x <- results$newlevels
      TEMP <- data.frame(y,x)
      mosaic(y~x,data=TEMP,xlab="New Levels",ylab=names(DATA)[2],inside=TRUE,equal=equal) } else {
        details$NewValue <- "A"
        results <- list(Details=details,newlevels=factor(rep("A",nrow(DATA))))
      }
      return(results) }
}





```


## Overview

Every year, approximately 7.6 million companion animals end up in US shelters. Many animals are given up as unwanted by their owners, while others are picked up after getting lost or taken out of cruelty situations. Many of these animals find forever families to take them home, but just as many are not so lucky. 2.7 million dogs and cats are euthanized in the US every year.

Using a dataset of intake information including breed, color, sex, and age from the Austin Animal Center, we're asking Kagglers to predict the outcome for each animal.  We also believe this dataset can help us understand trends in animal outcomes. These insights could help shelters focus their energy on specific animals who need a little extra help finding a new home.   

The dataset is brought to you by Austin Animal Center. Shelter animal statistics were taken from the ASPCA.  The data comes from Austin Animal Center from October 1st, 2013 to March, 2016. Outcomes represent the status of animals as they leave the Animal Center. All animals receive a unique Animal ID during intake. 


## Assignment

Load in the `AnimalOutcomesPhase3.RData` where you will find `TRAIN` and `HOLDOUT.TO.SHARE`.  Your goal is to determine the best predictive model using `TRAIN`, then make and submit predictions for the animals in `HOLDOUT.TO.SHARE`.

```{r load in}
load("AnimalOutcomesPhase3.RData")
```
Note 1:  you might want to combine infrequent levels.  The example below shows how you would do this with `combine_infrequent_levels` (provided above) on the `AgeuponOutcome` variable (not one I'd recommend doing this with though).

```{r combine more}
ALL <- rbind(TRAIN,HOLDOUT.TO.SHARE)
ALL$AgeuponOutcome <- combine_infrequent_levels(ALL$AgeuponOutcome,threshold=400)$values
ALL$Breed <- combine_infrequent_levels(ALL$Breed,threshold=400)$values
ALL$Color <- combine_infrequent_levels(ALL$Color,threshold=400)$values
NEW.TRAIN <- ALL[ (1:nrow(TRAIN)), ]
NEW.HOLDOUT <- ALL[ -(1:nrow(TRAIN)), ]
```

Note 2:  you might want to further combine levels in a supervised way.  The example below shows how you would do this with `discretize_x_for_categorical_y` (provided above) on the `AgeuponOutcome` variable (not one I'd recommend doing this with though).

```{r combine with mdlp}
library(discretization)
ALL <- rbind(TRAIN,HOLDOUT.TO.SHARE)
TO.DISCRETIZE <- ALL[,c("Breed","OutcomeType")]  #1st column x, 2nd column what you want to use to discretize
DC <- discretize_x_for_categorical_y(TO.DISCRETIZE,threshold=0,train.rows=1:nrow(TRAIN),equal=FALSE) 
ALL$Breed <- DC$newlevels

TO.DISCRETIZE <- ALL[,c("Color","OutcomeType")]  #1st column x, 2nd column what you want to use to discretize
DC <- discretize_x_for_categorical_y(TO.DISCRETIZE,threshold=0,train.rows=1:nrow(TRAIN),equal=FALSE) 
ALL$Breed <- DC$newlevels

NEW.TRAIN <- ALL[ (1:nrow(TRAIN)), ]
NEW.HOLDOUT <- ALL[ -(1:nrow(TRAIN)), ]
```

Note 3:  use the entirety of the training data when estimating the generalization error (you want to maximize AUC), i.e., there is no additional holdout sample.  Set up the following to do 5-fold crossvalidation.

```{r crossvalidation}
fitControl <- trainControl(method="cv",number=5, verboseIter = TRUE,
                           summaryFunction = twoClassSummary, classProbs = TRUE) 
```


Note 4:  It's useful (though not required) to allow your computer to do this in parallel.  This requires the `parallel` and `doParallel` packages.  However, it may not work with your version of `caret`.  In which case you can omit all lines that refer to `cluster`, `registerDoParallel`, `stopCluster(cluster)`, `registerDoSEQ()`


Note 5:  Below is an example of fitting the regularized logistic regression model with some "throw away" values of the tuning parameters.  Try out different models, different tuning parameters, etc., and see which has the highest estimated AUC on new data (the ROC value in )


```{r glmnet example}
seed <- 479
paramGrid <- expand.grid(.alpha = seq(0,1,.1),.lambda = 10^seq(-5,-1,length=5))  

########################################
####For parallelization (if it works)
########################################
#require(parallel)
#require(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
fitControl <- trainControl(method="cv",number=5, verboseIter = TRUE,
                           summaryFunction = twoClassSummary, classProbs = TRUE, allowParallel = TRUE) 
set.seed(seed);  GLMnet <- train(OutcomeType~.,data=NEW.TRAIN,metric="ROC",method='glmnet', 
                                 trControl=fitControl, tuneGrid=paramGrid)
#stopCluster(cluster)
#registerDoSEQ()
########################################
########################################



########################################
####Nonparallel way
########################################
fitControl <- trainControl(method="cv",number=5, verboseIter = TRUE,
                           summaryFunction = twoClassSummary, classProbs = TRUE) 
set.seed(seed);  GLMnet <- train(OutcomeType~.,data=NEW.TRAIN,metric="ROC",method='glmnet', 
                                 trControl=fitControl, tuneGrid=paramGrid)



#####See estimated ROC
GLMnet$bestTune
GLMnet$results[rownames(GLMnet$bestTune),] #0.9253187 for me
#0.9068603

####Make predictions
GLMnet.predictions <- predict(GLMnet,newdata=NEW.HOLDOUT,type="prob")



###What about without combining levels?

#require(parallel)
#require(doParallel)
#cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
#registerDoParallel(cluster)
fitControl <- trainControl(method="cv",number=5, verboseIter = TRUE,
                           summaryFunction = twoClassSummary, classProbs = TRUE, allowParallel = TRUE) 
set.seed(seed);  GLMnet.raw <- train(OutcomeType~.,data=TRAIN,metric="ROC",method='glmnet', 
                                 trControl=fitControl, tuneGrid=paramGrid)
#stopCluster(cluster)
registerDoSEQ()

GLMnet.raw$bestTune
GLMnet.raw$results[rownames(GLMnet.raw$bestTune),] #0.9344347 for me, SD of 0.0107
#0.9344376

####Make predictions
GLMnetraw.predictions <- predict(GLMnet.raw,newdata=HOLDOUT.TO.SHARE,type="prob")
write.csv(GLMnetraw.predictions,file="Kwoods21FinalPredictions.csv",row.names = FALSE)
#Looks like raw might be the way to go
```

Partition model (agnostic to transformations or scaling)

```{r partition}
#Set up search grid; values of cp we want to try out
rpartGrid <- expand.grid(cp=10^seq(from=-4,to=-1,length=30))
#train function
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
fitControl <- trainControl(method="cv",number=5, verboseIter = TRUE,
                           summaryFunction = twoClassSummary, classProbs = TRUE, allowParallel = TRUE) 
set.seed(474); RPARTfit <- train(OutcomeType~.,data=TRAIN,method="rpart",metric="ROC",trControl=fitControl,tuneGrid=rpartGrid)
stopCluster(cluster)
registerDoSEQ()

RPARTfit  #Look at output to see which cp was best
RPARTfit$bestTune #Gives best parameters
RPARTfit$results #Look at output in more detail (lets you see SDs)
RPARTfit$results[rownames(RPARTfit$bestTune),] #0.8749798  #not so good
plot(ROC~cp,data=RPARTfit$results,log="x")  #If tuned on AUC

RPART.predictions <- predict(RPARTfit,newdata=HOLDOUT.TO.SHARE,type="prob")

```

Random Forest (rarely need to tune mtry)

```{r partition}
#Set up search grid; values of cp we want to try out
forestGrid <- expand.grid(mtry=c(1,2,3,7))
#train function
#cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
#registerDoParallel(cluster)
fitControl <- trainControl(method="cv",number=5, verboseIter = TRUE,
                           summaryFunction = twoClassSummary, classProbs = TRUE, allowParallel = TRUE) 
set.seed(474); FORESTfit <- train(OutcomeType~.,data=TRAIN,method="rf",metric="ROC",trControl=fitControl,tuneGrid=forestGrid)
#stopCluster(cluster)
#registerDoSEQ()

FORESTfit  #Look at output to see which cp was best
FORESTfit$bestTune #Gives best parameters
FORESTfit$results #Look at output in more detail (lets you see SDs)
FORESTfit$results[rownames(FORESTfit$bestTune),] #0.9128592
plot(ROC~mtry,data=FORESTfit$results)  #If tuned on AUC
varImp(FORESTfit)

FOREST.predictions <- predict(FORESTfit,newdata=HOLDOUT.TO.SHARE,type="prob")

##What about on combined levels?
#cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
#registerDoParallel(cluster)
fitControl <- trainControl(method="cv",number=5, verboseIter = TRUE,
                           summaryFunction = twoClassSummary, classProbs = TRUE, allowParallel = TRUE) 
set.seed(474); FORESTfit.new <- train(OutcomeType~.,data=NEW.TRAIN,method="rf",metric="ROC",trControl=fitControl,tuneGrid=forestGrid)
#stopCluster(cluster)
registerDoSEQ()

FORESTfit.new  #Look at output to see which cp was best
FORESTfit.new$bestTune #Gives best parameters
FORESTfit.new$results #Look at output in more detail (lets you see SDs)
FORESTfit.new$results[rownames(FORESTfit.new$bestTune),] #0.8962562
plot(ROC~mtry,data=FORESTfit.new$results)  #If tuned on AUC
varImp(FORESTfit.new)


FOREST.predictions <- predict(FORESTfit.new,newdata=NEW.HOLDOUT,type="prob")

```

Boosted tree?

```{r boosted tree}
#Set up search grid; in general, smaller values of shrinkage are better, but this requires more trees
gbmGrid <- expand.grid(n.trees=c(500,1000,2000),
                       interaction.depth=2:4,
                       shrinkage=c(.005,.01),
                       n.minobsinnode=c(5,10))


#cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
#registerDoParallel(cluster)
fitControl <- trainControl(method="cv",number=5, verboseIter = TRUE,
                           summaryFunction = twoClassSummary, classProbs = TRUE, allowParallel = TRUE) 
set.seed(474); GBMfit <- train(OutcomeType~.,data=TRAIN,method="gbm",metric="ROC",trControl=fitControl,tuneGrid=gbmGrid,verbose=FALSE)
#stopCluster(cluster)
#registerDoSEQ()


#Look at results sorted from best to worst
GBMfit$bestTune  #best parameter set
GBMfit$results[order(GBMfit$results$ROC,decreasing=TRUE), ] #If tuned AUC
GBMfit$results[rownames(GBMfit$bestTune),] #0.9326295
varImp(GBMfit)


```


